---
title: "R Notebook"
output: html_notebook
---

Importing libraries.

```{r}
library(tidyverse)
library(readxl)
library(ggcorrplot)
library(broom)
library(MASS)
library(car)
library(lmtest)
library(olsrr)
```

Reading in the data.

```{r}
(bike <- read_xlsx("./SeoulBikeData.xlsx"))
```
## DATA ANALYSIS

Extracting and cleaning data
```{r}
# Response variable
rbc <- bike$`Rented Bike Count`

# Explanatory variables
hour <- bike$Hour
temp <- bike$`Temperature(°C)`
humid <- bike$`Humidity(%)`
windspd <- bike$`Wind speed (m/s)`
vis <- bike$`Visibility (10m)`
dptemp <- bike$`Dew point temperature(°C)`
solrad <- bike$`Solar Radiation (MJ/m2)`
rain <- bike$`Rainfall(mm)`
snow <- bike$`Snowfall (cm)`

# Categorical varaibles
season <- as.numeric(str_replace_all(bike$Seasons, c("Winter" = "1", "Spring" = "2", "Summer" = "3", "Autumn" = "4")))
holiday <- as.numeric(str_replace_all(bike$Holiday, c("No Holiday" = "0", "Holiday" = "1")))
funcday <- as.numeric(str_replace_all(bike$`Functioning Day`, c("Yes" = "1", "No" = "0")))

# Alternate df w/ factors replaced with numbers
bike_factor <- bike

bike_factor$Seasons <- as.numeric(str_replace_all(bike_factor$Seasons, c("Winter" = "1", "Spring" = "2", "Summer" = "3", "Autumn" = "4")))
bike_factor$Holiday <- as.numeric(str_replace_all(bike_factor$Holiday, c("No Holiday" = "0", "Holiday" = "1")))
bike_factor$`Functioning Day` <- as.numeric(str_replace_all(bike_factor$`Functioning Day`, c("Yes" = "1", "No" = "0")))
```

Plotting individual variables
```{r}
# Combining values from bike df for facet wrap plotting
cont_vars <- bike %>% relocate(Hour, .after = last_col()) %>% pivot_longer(`Rented Bike Count`:`Snowfall (cm)`, names_to = "xname", values_to = "x")

# Requires categorical variables to be converted to numbers, see bike_factor below
cat_vars <- bike_factor %>% dplyr::select(`Rented Bike Count`, Hour, Seasons, Holiday, `Functioning Day`) %>% pivot_longer(-`Rented Bike Count`, names_to = "xname", values_to = "x") 

# Continuous variables
# Histogram plots
ggplot(cont_vars, aes(x = x)) + geom_histogram() + facet_wrap(~xname, scales = "free")
ggplot(cont_vars, aes(x = x)) + geom_boxplot() + facet_wrap(~xname, scales = "free")

# Categorical variables
# Bar plot
ggplot(cat_vars, aes(x = x)) + geom_bar() + facet_wrap(~xname, scales = "free")
```

Plotting response variable against others
```{r, fig.height=10}
# Combining vars w/ rbc
cont_vars_rbc <- bike %>% pivot_longer(`Temperature(°C)`:`Snowfall (cm)`, names_to = "xname", values_to = "x")

# Cont. vars vs rbc
# Scatter plot
cont_vars_rbc %>% ggplot(aes(x = x, y = `Rented Bike Count`)) + geom_point() + facet_wrap(~xname, scales = "free")

# Cat. vars vs rbc
# Scatter plot
cat_vars %>% ggplot(aes(x = x, y = `Rented Bike Count`)) + geom_point() + facet_wrap(~xname, scales = "free")
```

Plotting correlation between explanatory variables

```{r}
# For cont. ex. vars
correlation_matrix <- cor(cbind(rbc, hour, temp, humid, windspd, vis, dptemp, solrad, rain, snow))

ggcorrplot(correlation_matrix)
```

# MODEL BUILDING

Simple linear model with full effect

```{r}
model1 <- lm(`Rented Bike Count` ~ .-Date, data = bike)
summary(model1)
```

- R-squared can be better
- Most variables seem significant except visibility

Try removing highly correlated variables as seen in correlation matrix

```{r}
model2 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`, data = bike)
summary(model2)
```

- Minimal change to R-squared and adj. R-squared
- P-values stayed significant if they already were
- Snowfall increased in p-value to 0.11 from 0.03, choose to keep for $\alpha=0.2$ for now

Drop other correlated variable dptemp

```{r}
model2.1 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`, data = bike)
summary(model2.1)
```
Now solrad and snow have become insignificant, and R-squared values dropping by 0.02
Drop solrad first, then check and drop snow if needed

```{r}
model2.2 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`, data = bike)
summary(model2.2)

model2.3 <- lm(`Rented Bike Count` ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = bike)
summary(model2.3)
```


A look at residual plots for further ideas

```{r, fig.height=10, fig}
# Add values from data frame into linear model for plotting against residuals
model2.3 %>% augment(bike) -> model2.3a

# Modifying data frame for plotting exp. vs resid. using facet wrap
model2.3a_long <- model2.3a %>% pivot_longer(c(Hour,`Temperature(°C)`, `Wind speed (m/s)`, `Visibility (10m)`, `Rainfall(mm)`), values_to = "x", names_to = "xname")

# Fitted vs residuals
ggplot(model2.3, aes(x = .fitted, y = .resid)) + geom_point()

# Explanatory vs residuals
ggplot(model2.3a_long, aes(x = x, y = .resid)) + geom_point() + facet_wrap(~xname, scales = "free")

# QQ plot against residuals
ggplot(model2.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()


bptest(model2.3)    # Breusch-Pagan test, reject constant variance
```

- Homoscedasticity seems apparent in both response and ex. vars.
- Some ex. vars. appear to be non-linear


```{r}
# Might remove this b/c it makes our model worse lol
# Use boxCox(model2.3, family="yjPower") w/ a$x[which.max(a$y)]

# YJ Transformation, Boxcox w/ neg. vals.
#biket <- bike %>% mutate(`Rented Bike Count` = yjPower(bike$`Rented Bike Count`, 2))

model3 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = bike)
summary(model3)

# Fitted vs residuals
ggplot(model3, aes(x = .fitted, y = .resid)) + geom_point()

# QQ plot against residuals
ggplot(model3, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

This transformation seems more inline with a good model, but we still have some outliers and this weird line on the left.

```{r}
model4 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`-`Rainfall(mm)`, data = bike)
summary(model4)

# Fitted vs residuals
ggplot(model4, aes(x = .fitted, y = .resid)) + geom_point()

# QQ plot against residuals
ggplot(model4, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

This does not work very well for us.

```{r}
model5 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Humidity(%)`-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`-`Functioning Day`, data = bike)
summary(model5)

# Fitted vs residuals
ggplot(model5, aes(x = .fitted, y = .resid)) + geom_point()

# QQ plot against residuals
ggplot(model5, aes(sample = .resid)) + stat_qq() + stat_qq_line()
```

## MODEL SELECTION
```{r}
#Subset selection

library(leaps)
allreg <- regsubsets(I(`Rented Bike Count`^0.1818) ~ .-Date, nbest=8, data=bike)
n = dim(bike)[1]
aprout = summary(allreg)
pprime = apply(aprout$which, 1, sum)
aprout$aic <- aprout$bic - log(n) * pprime + 2 * pprime
df = data.frame(with(aprout, round(cbind(which, rsq, adjr2, cp, bic, aic), 3)))
df[which.max(df$aic), ] #Model with lowest AIC
df[which.max(df$adjr2), ] #Model with highest adjusted R^2
```


```{r}
library(MASS)
step1 = stepAIC(model4, direction="both")

library(MPV)
PRESS(model4)
PRESS(model3)
PRESS(model2)
PRESS(model1)
```
```{r}
t = rstudent(model4)
alpha = 0.05

n = dim(bike)[1]
p.prime = length(coef(model4))

t.crit = qt(1 - alpha/(2*n), n - p.prime - 1)
round(t, 2)
```

```{r}
which(abs(t) > t.crit)
```
## MACHINE LEARNING MODELS

## TRAIN TEST SPLIT
```{r}
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
```

```{r}
data_train <- create_train_test(bike, size = 0.8, train = TRUE)
data_test <- create_train_test(bike, size = 0.8, train = FALSE)
dim(data_train)
dim(data_test)
```
## DECISION TREE
```{r}
library(caret)
library(rpart)
library(rpart.plot)
normalize <- function(x, na.rm = TRUE) {
    return((x- min(x)) /(max(x)-min(x)))
}

n_data_train <- (data_train %>% mutate(across(where(is.numeric), normalize)))
n_data_test <- (data_test %>% mutate(across(where(is.numeric), normalize)))
decisiontree <- rpart(`Rented Bike Count`~.-Date-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)` - `Functioning Day`, data = data_train, method = 'anova')
pred = predict(decisiontree, data_test, method = "anova")

#RMSE
RMSE(pred = pred, obs = data_test$`Rented Bike Count`)
#R^2
cor(data_test$`Rented Bike Count`, pred) ^ 2

x = 1:length(data_test$`Rented Bike Count`)
plot(x, data_test$`Rented Bike Count`, col = "red", type = "l")
lines(x, pred, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```

## RANDOM FOREST
```{r}
library(randomForest)
data_train2 <- data_train
data_test2 <- data_test
names(data_train2) <- make.names(names(data_train2))
names(data_test2) <- make.names(names(data_test2))
set.seed(222)
random_forest <- randomForest(`Rented.Bike.Count`~.-Date-`Dew.point.temperature..C.`-`Solar.Radiation..MJ.m2.`-`Snowfall..cm.` - `Functioning.Day`, data=data_train2, na.action=na.exclude)

random_forest
```

```{r}
p1 <- predict(random_forest, data_train2)
plot(p1)
p2 <- predict(random_forest, data_test2)

#RMSE
RMSE(pred = p2, obs = data_test2$`Rented.Bike.Count`)
#R^2
cor(data_test2$`Rented.Bike.Count`, p2) ^ 2

x = 1:length(data_test2$`Rented.Bike.Count`)
plot(x, data_test2$`Rented.Bike.Count`, col = "red", type = "l")
lines(x, p2, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```
## XGBOOST
```{r}
library(xgboost)
print(data_train[, names(data_train) != "Rented Bike Count"])
xgb_train = xgb.DMatrix(data = data.matrix(data_train[, names(data_train) != "Rented Bike Count"]), label = data_train$`Rented Bike Count`)
xgb_test = xgb.DMatrix(data = data.matrix(data_test[, names(data_test) != "Rented Bike Count"]), label = data_test$`Rented Bike Count`)
xgbr = xgboost(data=xgb_train, max.depth = 2, nrounds=50)
xgbr
```

```{r}
pred2 = predict(xgbr, xgb_test)

#RMSE
RMSE(pred2, data_test$`Rented Bike Count`)
# R^2
cor(data_test2$`Rented.Bike.Count`, pred2) ^ 2

x = 1:length(data_test$`Rented Bike Count`)
plot(x, data_test$`Rented Bike Count`, col = "red", type = "l")
lines(x, pred2, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```

## SUPPORT VECTOR REGRESSION
```{r}
#Load Library
library(e1071)
set.seed(222)

#Regression with SVM
n1_data_train = subset(n_data_train, select = -c(Seasons))
modelsvm <- svm(`Rented Bike Count`~.-Date-`Dew point temperature(°C)`-`Solar Radiation (MJ/m2)`-`Snowfall (cm)`, data = n1_data_train, type="eps-regression")
modelsvm

#Predict using SVM regression
predictions = predict(modelsvm, subset(n_data_test, select = -c(Seasons, `Rented Bike Count`)))

#RMSE
RMSE(n_data_test$`Rented Bike Count`, predictions)
#R^2
cor(n_data_test$`Rented Bike Count`, predictions) ^ 2
```

```{r}
x = 1:length(n_data_test$`Rented Bike Count`)
plot(x, n_data_test$`Rented Bike Count`, col = "red", type = "l")
lines(x, predictions, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```
##LightGBM
```{r}
library(lightgbm)
X=data.matrix(data_train[, names(data_train) != "Rented Bike Count"])
y=data.matrix(data_train[, names(data_train) == "Rented Bike Count"])
X_test=data.matrix(data_test[, names(data_test) != "Rented Bike Count"])
y_test=data.matrix(data_test[, names(data_test) == "Rented Bike Count"])

dtrain <- lgb.Dataset(X, label = y)
dtest <- lgb.Dataset.create.valid(dtrain, X_test, label = y_test)
model <- lightgbm(params = list(objective = "regression", metric = "l2"), data = dtrain)
lgbmpred = predict(model, X_test)
```
```{r}
#RMSE
RMSE(pred = lgbmpred, obs = data_test$`Rented Bike Count`)
#R^2
cor(data_test$`Rented Bike Count`, lgbmpred) ^ 2

x = 1:length(data_test$`Rented Bike Count`)
plot(x, data_test$`Rented Bike Count`, col = "red", type = "l")
lines(x, lgbmpred, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```
## KNN
```{r}
library(caret)
X2=data.matrix(n_data_train[, names(n_data_train) != "Rented Bike Count"])
y2=data.matrix(n_data_train[, names(n_data_train) == "Rented Bike Count"])
X_test2=data.matrix(n_data_test[, names(n_data_test) != "Rented Bike Count"])
y_test2=data.matrix(n_data_test[, names(n_data_test) == "Rented Bike Count"])
knnmodel = knnreg(X2, y2)
pred_y = predict(knnmodel, X_test2)
#RMSE
RMSE(n_data_test$`Rented Bike Count`, pred_y)
#R^2
cor(n_data_test$`Rented Bike Count`, pred_y) ^ 2

x = 1:length(data_test$`Rented Bike Count`)
plot(x, n_data_test$`Rented Bike Count`, col = "red", type = "l")
lines(x, pred_y, col = "blue", type = "l")
legend(x = "topleft", y = 38,  legend = c("original test_y", "predicted test_y"), 
       col = c("red", "blue"), box.lty = 1, cex = 0.8, lty = c(1, 1))
```

# FEATURE IMPORTANCE USING CARET
```{r}
set.seed(222)
library(mlbench)
importance <- varImp(model1, scale=FALSE)
print(importance)
plot(importance)

model6 <- lm(I(`Rented Bike Count`^0.1818) ~ .-Date-`Temperature(°C)`-`Dew point temperature(°C)`-`Visibility (10m)`-`Snowfall (cm)`-`Wind speed (m/s)`, data = bike)
summary(model6)
```